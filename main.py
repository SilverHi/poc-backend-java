#!/usr/bin/env python3
"""
Mock Research API - å•æ–‡ä»¶FastAPIæœåŠ¡
æä¾›ä¸€ä¸ªæ¨¡æ‹Ÿçš„ç ”ç©¶æŸ¥è¯¢æ¥å£ï¼Œä½¿ç”¨OpenAI 4o-miniç”Ÿæˆå›ç­”

ä½¿ç”¨æ–¹æ³•ï¼š
1. å®‰è£…ä¾èµ–: pip install fastapi uvicorn openai python-multipart pydantic
2. è®¾ç½®ç¯å¢ƒå˜é‡: export OPENAI_API_KEY="your-api-key"
3. è¿è¡ŒæœåŠ¡: python main.py

APIæ¥å£:
POST /research
- query: str (ç”¨æˆ·æŸ¥è¯¢)
- max_number: int (3 æˆ– 6ï¼Œæ§åˆ¶æ€è€ƒæ·±åº¦å’Œå“åº”æ—¶é—´)
"""

import asyncio
import random
import time
from typing import List, Dict, Any
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
import os

# å°è¯•å¯¼å…¥openaiï¼Œå¦‚æœå¤±è´¥åˆ™ä½¿ç”¨fallbackæ¨¡å¼
try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    print("è­¦å‘Š: OpenAIåº“æœªå®‰è£…ï¼Œå°†ä½¿ç”¨fallbackæ¨¡å¼")
    OPENAI_AVAILABLE = False

app = FastAPI(title="Mock Research API", version="1.0.0", description="æ¨¡æ‹Ÿç ”ç©¶æŸ¥è¯¢APIæœåŠ¡")

# OpenAIé…ç½®
if OPENAI_AVAILABLE:
    openai_api_key = os.getenv("OPENAI_API_KEY")
    if not openai_api_key:
        print("è­¦å‘Š: æœªè®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œå°†ä½¿ç”¨fallbackæ¨¡å¼")
        OPENAI_AVAILABLE = False

class QueryRequest(BaseModel):
    query: str
    max_number: int  # 3 or 6

class QueryResponse(BaseModel):
    result: str  # HTMLæ ¼å¼çš„å›ç­”
    raw_result: str  # ä¸å¸¦æ ¼å¼çš„åŸå§‹å›ç­”
    consume_token: int  # æ¶ˆè€—çš„tokenæ•°
    metadata: Dict[str, Any]  # metadataå¯¹è±¡
    chunk: List[str]  # å­—ç¬¦ä¸²æ•°ç»„ï¼Œç›¸å…³æ–‡æ¡£ç‰‡æ®µåŸæ–‡

@app.post("/research", response_model=QueryResponse)
async def research_query(request: QueryRequest):
    """
    ç ”ç©¶æŸ¥è¯¢çš„mockæ¥å£
    æ ¹æ®max_numberå‚æ•°æ¨¡æ‹Ÿä¸åŒçš„å“åº”æ—¶é—´ï¼š
    - max_number=3: 2-4åˆ†é’Ÿå“åº”æ—¶é—´
    - max_number=6: 3-6åˆ†é’Ÿå“åº”æ—¶é—´
    """
    
    # éªŒè¯max_numberå‚æ•°
    if request.max_number not in [3, 6]:
        raise HTTPException(status_code=400, detail="max_number must be 3 or 6")
    
    # æ ¹æ®max_numberè®¾ç½®å»¶è¿Ÿæ—¶é—´
    if request.max_number == 3:
        delay_seconds = random.randint(120, 240)  # 2-4åˆ†é’Ÿ
    else:  # max_number == 6
        delay_seconds = random.randint(180, 360)  # 3-6åˆ†é’Ÿ
    print(f"delay_seconds: {delay_seconds}")
    # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    # await asyncio.sleep(delay_seconds)
    
    # å°è¯•ä½¿ç”¨OpenAIç”ŸæˆçœŸå®å›ç­”
    if OPENAI_AVAILABLE:
        try:
            # ä½¿ç”¨OpenAI 4o-miniç”Ÿæˆå›ç­”
            client = openai.OpenAI(api_key=openai_api_key)
            
            # ç”ŸæˆåŸå§‹å›ç­”
            completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system", 
                        "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç ”ç©¶åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ç”¨æˆ·çš„æŸ¥è¯¢æä¾›è¯¦ç»†ä¸”æœ‰ç”¨çš„å›ç­”ã€‚å›ç­”åº”è¯¥å‡†ç¡®ã€å…¨é¢ä¸”æ˜“äºç†è§£ã€‚"
                    },
                    {
                        "role": "user", 
                        "content": f"è¯·è¯¦ç»†å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š{request.query}"
                    }
                ],
                max_tokens=2000,
                temperature=0.7
            )
            
            raw_result = completion.choices[0].message.content
            consume_token = completion.usage.total_tokens
            
            # å°†åŸå§‹å›ç­”è½¬æ¢ä¸ºHTMLæ ¼å¼
            html_result = convert_to_html(raw_result)
            
            # ç”Ÿæˆmockçš„æ–‡æ¡£ç‰‡æ®µ
            chunk_completion = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[
                    {
                        "role": "system",
                        "content": "è¯·ç”Ÿæˆ3-5ä¸ªä¸æŸ¥è¯¢ç›¸å…³çš„æ–‡æ¡£ç‰‡æ®µï¼Œæ¯ä¸ªç‰‡æ®µåº”è¯¥æ˜¯ç‹¬ç«‹çš„ä¿¡æ¯å—ã€‚"
                    },
                    {
                        "role": "user",
                        "content": f"ä¸ºè¿™ä¸ªæŸ¥è¯¢ç”Ÿæˆç›¸å…³æ–‡æ¡£ç‰‡æ®µï¼š{request.query}"
                    }
                ],
                max_tokens=800,
                temperature=0.5
            )
            
            chunk_text = chunk_completion.choices[0].message.content
            chunks = [chunk.strip() for chunk in chunk_text.split('\n\n') if chunk.strip()]
            
            # æ›´æ–°æ€»tokenæ¶ˆè€—
            consume_token += chunk_completion.usage.total_tokens
            
            # ç”Ÿæˆmetadata
            metadata = {
                "query_complexity": request.max_number,
                "processing_time": delay_seconds,
                "model_used": "gpt-4o-mini",
                "timestamp": int(time.time()),
                "confidence_score": random.uniform(0.7, 0.95),
                "sources_count": len(chunks)
            }
            
            return QueryResponse(
                result=html_result,
                raw_result=raw_result,
                consume_token=consume_token,
                metadata=metadata,
                chunk=chunks
            )
            
        except Exception as e:
            print(f"OpenAIè°ƒç”¨å¤±è´¥: {e}")
            # å¦‚æœOpenAIè°ƒç”¨å¤±è´¥ï¼Œè¿”å›mockæ•°æ®
            return generate_fallback_response(request.query, request.max_number, delay_seconds)
    else:
        # å¦‚æœOpenAIä¸å¯ç”¨ï¼Œç›´æ¥è¿”å›mockæ•°æ®
        return generate_fallback_response(request.query, request.max_number, delay_seconds)

def convert_to_html(text: str) -> str:
    """å°†çº¯æ–‡æœ¬è½¬æ¢ä¸ºHTMLæ ¼å¼"""
    if not text:
        return "<p>æ²¡æœ‰å†…å®¹</p>"
    
    # å¤„ç†æ®µè½åˆ†éš”
    paragraphs = text.split('\n\n')
    html_paragraphs = []
    
    for paragraph in paragraphs:
        if paragraph.strip():
            # å¤„ç†æ¢è¡Œ
            formatted_paragraph = paragraph.replace('\n', '<br>')
            html_paragraphs.append(f'<p>{formatted_paragraph}</p>')
    
    # ç»„åˆHTMLå†…å®¹
    content = '\n'.join(html_paragraphs)
    
    # æ·»åŠ ç¾è§‚çš„æ ·å¼
    html = f"""
    <div style="
        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; 
        line-height: 1.8; 
        color: #2c3e50; 
        max-width: 800px; 
        margin: 0 auto;
        padding: 20px;
        background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
        border-radius: 10px;
        box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);
    ">
        <div style="
            background: white; 
            padding: 25px; 
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0, 0, 0, 0.05);
        ">
            {content}
        </div>
    </div>
    """
    return html

def generate_fallback_response(query: str, max_number: int, delay_seconds: int) -> QueryResponse:
    """ç”Ÿæˆfallbackå“åº”ï¼ˆå½“OpenAIä¸å¯ç”¨æ—¶ï¼‰"""
    
    raw_result = f"""
    æ ¹æ®æ‚¨çš„æŸ¥è¯¢"{query}"ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¨¡æ‹Ÿçš„ç ”ç©¶ç»“æœã€‚
    
    åœ¨æ·±åº¦ä¸º{max_number}çš„åˆ†æä¸­ï¼Œæˆ‘ä»¬å‘ç°ä»¥ä¸‹å…³é”®ä¿¡æ¯ï¼š
    
    1. ç›¸å…³æ¦‚å¿µå’Œå®šä¹‰
    2. ä¸»è¦å½±å“å› ç´ 
    3. å½“å‰å‘å±•è¶‹åŠ¿
    4. å®é™…åº”ç”¨æ¡ˆä¾‹
    
    è¿™ä¸ªå›ç­”æ˜¯åŸºäºå¹¿æ³›çš„èµ„æ–™åˆ†æå’Œä¸“ä¸šåˆ¤æ–­å¾—å‡ºçš„ã€‚
    """
    
    html_result = convert_to_html(raw_result)
    
    chunks = [
        f"æ–‡æ¡£ç‰‡æ®µ1ï¼šå…³äº{query}çš„åŸºç¡€æ¦‚å¿µä»‹ç»...",
        f"æ–‡æ¡£ç‰‡æ®µ2ï¼š{query}çš„å†å²å‘å±•å’Œæ¼”å˜è¿‡ç¨‹...",
        f"æ–‡æ¡£ç‰‡æ®µ3ï¼š{query}åœ¨å®é™…åº”ç”¨ä¸­çš„æ¡ˆä¾‹åˆ†æ...",
        f"æ–‡æ¡£ç‰‡æ®µ4ï¼š{query}çš„æœªæ¥å‘å±•è¶‹åŠ¿å’Œå‰æ™¯..."
    ]
    
    metadata = {
        "query_complexity": max_number,
        "processing_time": delay_seconds,
        "model_used": "fallback-mock",
        "timestamp": int(time.time()),
        "confidence_score": 0.8,
        "sources_count": len(chunks)
    }
    
    return QueryResponse(
        result=html_result,
        raw_result=raw_result.strip(),
        consume_token=random.randint(800, 1500),
        metadata=metadata,
        chunk=chunks
    )

@app.get("/")
async def root():
    return {"message": "Mock Research API is running", "endpoints": ["/research"]}

@app.get("/health")
async def health_check():
    return {
        "status": "healthy", 
        "timestamp": int(time.time()),
        "openai_available": OPENAI_AVAILABLE,
        "version": "1.0.0"
    }

@app.get("/test")
async def test_endpoint():
    """å¿«é€Ÿæµ‹è¯•æ¥å£ï¼Œæ— å»¶è¿Ÿ"""
    test_query = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æŸ¥è¯¢"
    test_response = generate_fallback_response(test_query, 3, 0)
    return {
        "message": "æµ‹è¯•æ¥å£æ­£å¸¸",
        "sample_response": test_response,
        "note": "è¿™æ˜¯ä¸€ä¸ªå¿«é€Ÿæµ‹è¯•ï¼Œå®é™…/researchæ¥å£ä¼šæœ‰2-6åˆ†é’Ÿçš„å»¶è¿Ÿ"
    }

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ å¯åŠ¨Mock Research APIæœåŠ¡")
    print("=" * 60)
    print("ğŸ“Š APIæ–‡æ¡£åœ°å€: http://localhost:8000/docs")
    print("ğŸ¥ å¥åº·æ£€æŸ¥: http://localhost:8000/health")
    print("ğŸ” ç ”ç©¶æ¥å£: POST http://localhost:8000/research")
    print("-" * 60)
    print("ğŸ“‹ è¯·æ±‚ç¤ºä¾‹:")
    print('curl -X POST "http://localhost:8000/research" \\')
    print('  -H "Content-Type: application/json" \\')
    print('  -d \'{"query": "ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½?", "max_number": 3}\'')
    print("=" * 60)
    
    try:
        import uvicorn
        uvicorn.run(app, host="0.0.0.0", port=8000, log_level="info")
    except ImportError:
        print("âŒ é”™è¯¯: æœªå®‰è£…uvicorn")
        print("è¯·è¿è¡Œ: pip install uvicorn")
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥ç«¯å£8000æ˜¯å¦è¢«å ç”¨") 